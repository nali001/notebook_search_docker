{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build faiss indexes using haystack\n",
    "Ref: https://github.com/deepset-ai/haystack-tutorials/blob/main/tutorials/06_Better_Retrieval_via_Embedding_Retrieval.ipynb\n",
    "\n",
    "Ref: https://docs.haystack.deepset.ai/docs/retriever#documentstore-compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = '../data/evaluation/notebooks/notebooks_contents'\n",
    "DOCS_FILE = '../preprocessed_data/docs.json'\n",
    "FAISS_INDEX_DIR = './faiss_indexes_512'\n",
    "FAISS_DB_DIR = './faiss_db_512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = [(\"model1\", \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"),\n",
    "                    (\"model2\", \"sentence-transformers/all-mpnet-base-v2\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model2', 'sentence-transformers/all-mpnet-base-v2')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = EMBEDDING_MODELS[1]\n",
    "embedding_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing (First run only)\n",
    "- Transform the JSON file to input form\n",
    "- Convert JSON to `document`\n",
    "- Split the documents to passages\n",
    "- Index the passages to `document_store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3779] computational notebooks\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "# Set the path to the directory containing the input JSON files\n",
    "input_dir = NOTEBOOK_DIR\n",
    "\n",
    "# Set the path to the output JSON file\n",
    "output_file = DOCS_FILE\n",
    "\n",
    "# Loop through the input JSON files and extract relevant information to a new JSON file\n",
    "data = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "            data.append({\n",
    "                \"docid\": json_data[\"docid\"],\n",
    "                \"content\": json_data[\"md_text_clean\"],\n",
    "            })\n",
    "file_count = sum(1 for file in os.listdir(input_dir) if file.endswith('.json'))\n",
    "print(f\"[{file_count}] computational notebooks\")\n",
    "# Write the extracted data to the output JSON file\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/na/miniconda3/envs/vre/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3779"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.nodes import JsonConverter\n",
    "\n",
    "converter = JsonConverter()\n",
    "docs = converter.convert(DOCS_FILE)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723045a7e4324f2a9d7390c43c4c073e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/3779 [00:00<?, ?docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Document f955c4408695b9f350043e4a7fca3a83 is 24064 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 4404a3e98dfe4df3cbee942240d9a06e is 57280 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 6ea3852f87501e1f0e1fa201dffe34df is 26578 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 14383041fec20b28f2ca5ff56c1d524e is 234679 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 6a30a08dccc73af3339ab0606a76db4 is 146705 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 41aab7017ce76e1314ff6640b8242c13 is 77298 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document b344f2366acc284957e7db0577508a93 is 76845 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 944b1a3624c4534e1d301721ac7fc276 is 776911 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "We found one or more sentences whose word count is higher than the split length.\n",
      "Document 1945e833eed34277848d0d2df68c86a9 is 2716672 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 60dffee0c0a62eab321958a544d64ea0 is 143901 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document b344f2366acc284957e7db0577508a93 is 76845 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document db678281a3b99ca389f1ecd02ddb9b93 is 890160 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document f955c4408695b9f350043e4a7fca3a83 is 24064 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 133ca33421ae8f3278bd489be37c0af is 178743 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document a145a939ec1f897fc7e791c019f36c50 is 78891 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document d5f5de0da4b52727ae646a1cf3e81d8e is 172483 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 3d0ae91cbfdd01b7426a557ca66cd03d is 483173 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 8d75365a2b65d5479e78f8517bb9fadf is 228453 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 463bfb513ebd7b452dde2f990009b9a7 is 12708 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n",
      "Document 1945e833eed34277848d0d2df68c86a9 is 2716672 characters long after preprocessing, where the maximum length should be 10000. Something might be wrong with the splitting, check the document affected to prevent issues at query time.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4970"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "processor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=512,\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=0\n",
    ")\n",
    "\n",
    "passages = processor.process(docs)\n",
    "len(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Write documents\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "os.makedirs(f\"{FAISS_DB_DIR}/{embedding_model[0]}\", exist_ok=True)\n",
    "document_store = FAISSDocumentStore(sql_url=f\"sqlite:///{FAISS_DB_DIR}/{embedding_model[0]}/faiss_base.db\", faiss_index_factory_str = \"Flat\")\n",
    "\n",
    "for i, passage in enumerate(passages): \n",
    "    docid = passage.meta['docid']\n",
    "    passage_docid = f\"{docid}_passage{i}\"\n",
    "    index_document = {\n",
    "        \"id\": passage_docid,\n",
    "        \"content\": passage.content,\n",
    "        \"meta\": {\n",
    "            \"name\": docid,\n",
    "            \"passage_number\": i,\n",
    "        },\n",
    "    }\n",
    "    document_store.write_documents([index_document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in dir(document_store): \n",
    "#     print(i)\n",
    "\n",
    "document_store.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_document_count(), document_store.get_embedding_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update embeddings\n",
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "def update_index(document_store, embedding_model):\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store,\n",
    "        embedding_model=embedding_model[1],\n",
    "    )\n",
    "    # Important:\n",
    "    # Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "    # previously indexed documents and update their embedding representation.\n",
    "    # While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "    # At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "    document_store.update_embeddings(retriever)\n",
    "\n",
    "    # Save the document store:\n",
    "    index_path=f\"{FAISS_INDEX_DIR}/{embedding_model[0]}/index.faiss\"\n",
    "    config_path=f\"{FAISS_INDEX_DIR}/{embedding_model[0]}/config.json\"\n",
    "    os.makedirs(f\"{FAISS_INDEX_DIR}/{embedding_model[0]}\", exist_ok=True)\n",
    "    \n",
    "    document_store.save(index_path=index_path, config_path=config_path)\n",
    "    print(f\"Save index to {index_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927cd019e9d54640b19f78dafcdf45ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/4970 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abe443ee7644bb9bd519b4f56b2f38e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save index to ./faiss_indexes_512/model2/index.faiss\n"
     ]
    }
   ],
   "source": [
    "update_index(document_store, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4970, 4970)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.get_document_count(), document_store.get_embedding_count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = EMBEDDING_MODELS[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "index_path=f\"{FAISS_INDEX_DIR}/{embedding_model[0]}/index.faiss\"\n",
    "config_path=f\"{FAISS_INDEX_DIR}/{embedding_model[0]}/config.json\"\n",
    "document_store = FAISSDocumentStore.load(index_path=index_path, config_path=config_path)\n",
    "\n",
    "# Check if the DocumentStore is loaded correctly\n",
    "assert document_store.faiss_index_factory_str == \"Flat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4970"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# document_store.get_document_count()\n",
    "document_store.get_embedding_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
