{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2d14c01",
   "metadata": {},
   "source": [
    "## Compute the ideal values for the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37d9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QRELS_DIR = '../data/annotation/results/qrels_merged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3414847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_zero(lst):\n",
    "    count = 0\n",
    "    for num in lst:\n",
    "        if num != 0:\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6383d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EQ_10': 8, 'EQ_38': 5, 'EQ_23': 6, 'EQ_22': 4, 'EQ_2': 4, 'EQ_28': 19, 'EQ_21': 5, 'EQ_25': 5, 'EQ_32': 3, 'EQ_14': 3, 'EQ_44': 5, 'EQ_16': 17, 'EQ_5': 2, 'EQ_4': 6, 'EQ_35': 8, 'EQ_24': 11, 'EQ_34': 11, 'EQ_19': 5, 'EQ_1': 5, 'EQ_29': 6, 'EQ_9': 6, 'EQ_27': 4, 'EQ_37': 2, 'EQ_42': 8, 'EQ_12': 9, 'EQ_48': 4, 'EQ_26': 4, 'EQ_45': 1, 'EQ_49': 4, 'EQ_33': 6, 'EQ_47': 5, 'EQ_13': 3, 'EQ_18': 3, 'EQ_40': 7, 'EQ_3': 7, 'EQ_17': 2, 'EQ_46': 9, 'EQ_6': 1, 'EQ_20': 10, 'EQ_8': 2, 'EQ_11': 2, 'EQ_15': 3, 'EQ_43': 2, 'EQ_39': 1, 'EQ_36': 3, 'EQ_7': 6, 'EQ_30': 2}\n",
      "5.404255319148936\n",
      "1\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Statistics for relevance judgements\n",
    "import os\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "def count_judgements(directory):\n",
    "    judgements_count = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath) as file:\n",
    "                data = json.load(file)\n",
    "                qid = filename.split(\".\")[0]\n",
    "                judgements = data.get('judgements', [])\n",
    "                judgements_count[qid] = len(judgements)\n",
    "\n",
    "    return judgements_count\n",
    "\n",
    "directory_path = QRELS_DIR\n",
    "result = count_judgements(directory_path)\n",
    "print(result)\n",
    "\n",
    "\n",
    "judgements = list(result.values())\n",
    "print(statistics.mean(judgements))\n",
    "print(min(judgements))\n",
    "print(max(judgements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f75eecd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics@10\n",
      "Average Precision: 0.4191\n",
      "Average Recall: 0.9856\n",
      "Average F1 Score: 0.5356\n",
      "# queries: 47\n",
      "# judgements: 254\n",
      "# relevant: 208\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set the value of k for computing metrics@k\n",
    "k = 10\n",
    "\n",
    "# Read in the relevance judgments from the first directory\n",
    "judgements = {}\n",
    "judgement_count = 0\n",
    "relevant_count = 0\n",
    "for json_file in os.listdir(QRELS_DIR):\n",
    "    if not json_file.endswith(\".json\"):\n",
    "        continue\n",
    "    with open(os.path.join(QRELS_DIR, json_file)) as f:\n",
    "        data = json.load(f)\n",
    "        qid = data['qid']\n",
    "        labels = [item['relevance'] for item in data['judgements']]\n",
    "        labels.sort(reverse=True)\n",
    "        judgements[qid] = labels\n",
    "        judgement_count += len(labels)\n",
    "        relevant_count += count_non_zero(labels)\n",
    "\n",
    "# Calculate evaluation metrics@k for each query\n",
    "ideal_precisions = []\n",
    "ideal_recalls = []\n",
    "ideal_f1_scores = []\n",
    "ideal_ndcgs = []\n",
    "for qid in judgements.keys():\n",
    "    # Extract the relevance scores for the ranked documents\n",
    "    relevance_scores = judgements[qid]\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = min(k, count_non_zero(relevance_scores)) / k if k > 0 else 0\n",
    "    recall = min(k, count_non_zero(relevance_scores)) / count_non_zero(relevance_scores) if count_non_zero(relevance_scores) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # # Check if all relevance scores are zero\n",
    "    # if sum(relevance_scores) == 0:\n",
    "    #     ndcg = 0\n",
    "    # else:\n",
    "    #     # Calculate NDCG@k\n",
    "    #     dcg = relevance_scores[0]\n",
    "    #     for i in range(1, min(len(relevance_scores), k)):\n",
    "    #         dcg += relevance_scores[i] / np.log2(i+1)\n",
    "    #     idcg = sorted(relevance_scores, reverse=True)\n",
    "    #     idcg = idcg[0] + sum([idcg[i] / np.log2(i+1) for i in range(1, min(len(idcg), k))])\n",
    "    #     ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    \n",
    "    # Append the evaluation metrics to the lists\n",
    "    ideal_precisions.append(precision)\n",
    "    ideal_recalls.append(recall)\n",
    "    ideal_f1_scores.append(f1)\n",
    "    # all_ndcgs.append(ndcg)\n",
    "\n",
    "# Compute the average evaluation metrics@k across all queries\n",
    "avg_precision = np.mean(ideal_precisions)\n",
    "avg_recall = np.mean(ideal_recalls)\n",
    "avg_f1_score = np.mean(ideal_f1_scores)\n",
    "# avg_ndcg = np.mean(all_ndcgs)\n",
    "\n",
    "# Print the evaluation metrics@k\n",
    "print(f\"Metrics@{k}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "# print(f\"Average NDCG: {avg_ndcg:.4f}\")\n",
    "\n",
    "print(f\"# queries: {len(judgements)}\")\n",
    "print(f\"# judgements: {judgement_count}\")\n",
    "print(f\"# relevant: {relevant_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3057e07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ideal_precisions: \n",
    "    if item==0: \n",
    "        print('zero')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ac44621-86f3-401f-8691-092e0870f165",
   "metadata": {},
   "source": [
    "# Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a73b379-4220-4dc0-bae4-7c456ea96d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QRELS_DIR = '../data/annotation/results/qrels_merged'\n",
    "\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text/bm25'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text/model1'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text/model2'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code/bm25'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code/model1'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code/model2'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code/model3'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code/model4'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/code_comments/bm25'\n",
    "RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text_code/bm25'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text_code/model1'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/text_code/model2'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/fusion/bm25_model1'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/fusion/bm25_model2'\n",
    "# RESULTS_DIR = '../data/evaluation/results/evaluation_queries/fusion/model1_model2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3026a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text\n",
    "# EMBEDDING_MODELS = [(\"model1\", \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"),\n",
    "#                      (\"model2\", \"sentence-transformers/all-mpnet-base-v2\")]\n",
    "\n",
    "# Code\n",
    "EMBEDDING_MODELS = [(\"model1\", \"microsoft/codebert-base\"), \n",
    "                    (\"model2\", \"flax-sentence-embeddings/st-codesearch-distilroberta-base\"), \n",
    "                    (\"model3\", \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"),\n",
    "                    (\"model4\", \"sentence-transformers/all-mpnet-base-v2\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b12b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_lists(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must have the same length.\")\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(list1)):\n",
    "        # Perform division element-wise\n",
    "        if list2[i] != 0:\n",
    "            result.append(list1[i] / list2[i])\n",
    "        else:\n",
    "            # Handle division by zero\n",
    "            result.append(float('inf'))\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5de1014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/evaluation/results/evaluation_queries/text_code/bm25'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5af96a10-5af7-4b8f-9a49-c04f87cee29c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ_10 [0, 0, 0, 0, 0]\n",
      "EQ_38 [0, 0, 0, 2, 0]\n",
      "EQ_23 [2, 0, 0, 0, 0]\n",
      "EQ_22 [2, 0, 1, 1, 0]\n",
      "EQ_2 [0, 0, 2, 1, 2]\n",
      "EQ_28 [0, 0, 0, 0, 0]\n",
      "EQ_21 [0, 0, 0, 0, 0]\n",
      "EQ_25 [0, 0, 1, 2, 0]\n",
      "EQ_32 [0, 0, 0, 0, 1]\n",
      "EQ_14 [0, 0, 1, 2, 0]\n",
      "EQ_44 [0, 0, 1, 0, 0]\n",
      "EQ_16 [0, 0, 0, 0, 0]\n",
      "EQ_5 [0, 0, 0, 0, 2]\n",
      "EQ_4 [0, 0, 2, 0, 0]\n",
      "EQ_35 [0, 0, 0, 1, 0]\n",
      "EQ_24 [2, 2, 0, 2, 0]\n",
      "EQ_34 [0, 3, 0, 0, 3]\n",
      "EQ_19 [1, 0, 0, 0, 0]\n",
      "EQ_1 [2, 0, 2, 2, 0]\n",
      "EQ_29 [1, 0, 0, 3, 2]\n",
      "EQ_9 [3, 0, 2, 2, 3]\n",
      "EQ_27 [0, 0, 0, 0, 0]\n",
      "EQ_37 [0, 0, 0, 0, 0]\n",
      "EQ_42 [0, 0, 0, 1, 0]\n",
      "EQ_12 [0, 0, 0, 0, 0]\n",
      "EQ_48 [2, 0, 0, 0, 0]\n",
      "EQ_26 [0, 3, 0, 2, 0]\n",
      "EQ_45 [0, 0, 0, 0, 0]\n",
      "EQ_49 [0, 0, 1, 0, 0]\n",
      "EQ_33 [3, 2, 2, 2, 0]\n",
      "EQ_47 [2, 2, 2, 2, 0]\n",
      "EQ_13 [0, 0, 0, 0, 0]\n",
      "EQ_18 [0, 0, 0, 0, 0]\n",
      "EQ_40 [0, 3, 0, 3, 3]\n",
      "EQ_3 [0, 0, 0, 0, 0]\n",
      "EQ_17 [0, 0, 0, 0, 0]\n",
      "EQ_46 [2, 0, 0, 0, 0]\n",
      "EQ_6 [0, 0, 0, 0, 0]\n",
      "EQ_20 [0, 2, 3, 0, 2]\n",
      "EQ_8 [1, 0, 0, 0, 1]\n",
      "EQ_11 [0, 0, 0, 0, 0]\n",
      "EQ_15 [0, 0, 0, 0, 2]\n",
      "EQ_43 [0, 0, 2, 0, 0]\n",
      "EQ_39 [0, 0, 0, 0, 0]\n",
      "EQ_36 [0, 0, 2, 0, 0]\n",
      "EQ_7 [2, 2, 0, 0, 0]\n",
      "EQ_30 [2, 0, 0, 0, 0]\n",
      "Metrics@5\n",
      "Average Precision: 0.2596\n",
      "Average Recall: 0.3422\n",
      "Average F1 Score: 0.2730\n",
      "Average NDCG: 0.5584\n",
      "\n",
      "EQ_10 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_38 [0, 0, 0, 2, 0, 0, 0, 0, 0, 0]\n",
      "EQ_23 [2, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "EQ_22 [2, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "EQ_2 [0, 0, 2, 1, 2, 0, 0, 0, 0, 0]\n",
      "EQ_28 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_21 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_25 [0, 0, 1, 2, 0, 0, 2, 0, 0, 0]\n",
      "EQ_32 [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "EQ_14 [0, 0, 1, 2, 0, 0, 0, 0, 0, 0]\n",
      "EQ_44 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_16 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_5 [0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
      "EQ_4 [0, 0, 2, 0, 0, 1, 2, 2, 0, 0]\n",
      "EQ_35 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "EQ_24 [2, 2, 0, 2, 0, 0, 2, 0, 0, 2]\n",
      "EQ_34 [0, 3, 0, 0, 3, 0, 0, 0, 3, 0]\n",
      "EQ_19 [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_1 [2, 0, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "EQ_29 [1, 0, 0, 3, 2, 0, 0, 3, 2, 0]\n",
      "EQ_9 [3, 0, 2, 2, 3, 2, 0, 0, 0, 0]\n",
      "EQ_27 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "EQ_37 [0, 0, 0, 0, 0, 2, 0, 0, 0, 2]\n",
      "EQ_42 [0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
      "EQ_12 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_48 [2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_26 [0, 3, 0, 2, 0, 1, 0, 0, 0, 0]\n",
      "EQ_45 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_49 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_33 [3, 2, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "EQ_47 [2, 2, 2, 2, 0, 0, 0, 0, 0, 0]\n",
      "EQ_13 [0, 0, 0, 0, 0, 0, 2, 2, 0, 0]\n",
      "EQ_18 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_40 [0, 3, 0, 3, 3, 0, 3, 3, 0, 0]\n",
      "EQ_3 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_17 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_46 [2, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "EQ_6 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_20 [0, 2, 3, 0, 2, 2, 2, 2, 0, 0]\n",
      "EQ_8 [1, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "EQ_11 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_15 [0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
      "EQ_43 [0, 0, 2, 0, 0, 0, 0, 2, 0, 0]\n",
      "EQ_39 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "EQ_36 [0, 0, 2, 0, 0, 0, 3, 0, 0, 2]\n",
      "EQ_7 [2, 2, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "EQ_30 [2, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "Metrics@10\n",
      "Average Precision: 0.1915\n",
      "Average Recall: 0.5078\n",
      "Average F1 Score: 0.2593\n",
      "Average NDCG: 0.5720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set the value of k for computing metrics@k\n",
    "for k in [5, 10]: \n",
    "\n",
    "    # Read in the relevance judgments from the first directory\n",
    "    judgements = {}\n",
    "    for json_file in os.listdir(QRELS_DIR):\n",
    "        if not json_file.endswith(\".json\"):\n",
    "            continue\n",
    "        with open(os.path.join(QRELS_DIR, json_file)) as f:\n",
    "            data = json.load(f)\n",
    "            qid = data['qid']\n",
    "            judgements[qid] = {item['docid']: item['relevance'] for item in data['judgements']}\n",
    "\n",
    "    # Read in the ranking results from the second directory\n",
    "    results = {}\n",
    "    for json_file in os.listdir(RESULTS_DIR):\n",
    "        if not json_file.endswith(\".json\"):\n",
    "            continue\n",
    "        with open(os.path.join(RESULTS_DIR, json_file)) as f:\n",
    "            data = json.load(f)\n",
    "            qid = data['qid']\n",
    "            results[qid] = [(item['docid'], item['score']) for item in data['docs']]\n",
    "\n",
    "    # Calculate evaluation metrics@k for each query\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_f1_scores = []\n",
    "    all_ndcgs = []\n",
    "\n",
    "    norm_precisions = []\n",
    "    norm_recalls = []\n",
    "    norm_f1_scores = []\n",
    "\n",
    "    for qid in judgements.keys():\n",
    "        # Sort the ranked list of documents by score in descending order\n",
    "        ranked_docs = sorted(results[qid], key=lambda x: x[1], reverse=True)[:k]\n",
    "        # Extract the document IDs from the ranked list\n",
    "        ranked_docids = [doc[0] for doc in ranked_docs]\n",
    "        # Extract the relevance scores for the ranked documents\n",
    "        relevance_scores = [judgements[qid].get(docid, 0) for docid in ranked_docids]\n",
    "        \n",
    "        print(qid, relevance_scores)\n",
    "        # Calculate precision, recall, and F1 score\n",
    "        tp = count_non_zero(relevance_scores)\n",
    "        fp = len(relevance_scores) - tp\n",
    "        precision = tp / k if k > 0 else 0\n",
    "        recall = tp / count_non_zero(judgements[qid].values()) if count_non_zero(judgements[qid].values()) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        data = judgements[qid]\n",
    "        ideal_scores = list(data.values())\n",
    "        ideal_scores.sort(reverse=True)\n",
    "        ideal_precision = min(k, count_non_zero(ideal_scores))/k if k > 0 else 0\n",
    "        ideal_recall = min(k, count_non_zero(ideal_scores)) / count_non_zero(ideal_scores) if count_non_zero(ideal_scores) > 0 else 0\n",
    "        ideal_f1 = 2 * (ideal_precision * ideal_recall) / (ideal_precision + ideal_recall) if (ideal_precision + ideal_recall) > 0 else 0\n",
    "\n",
    "        norm_precision = precision/ideal_precision\n",
    "        norm_recall = recall/ideal_recall\n",
    "        norm_f1 = f1/ideal_f1\n",
    "        \n",
    "        # Check if all relevance scores are zero\n",
    "        if count_non_zero(relevance_scores) == 0:\n",
    "            ndcg = 0\n",
    "        else:\n",
    "            # Calculate NDCG@k\n",
    "            dcg = relevance_scores[0]\n",
    "            for i in range(1, min(len(relevance_scores), k)):\n",
    "                dcg += relevance_scores[i] / np.log2(i+1)\n",
    "            idcg = sorted(relevance_scores, reverse=True)\n",
    "            idcg = idcg[0] + count_non_zero([idcg[i] / np.log2(i+1) for i in range(1, min(len(idcg), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "        \n",
    "        # Append the evaluation metrics to the lists\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "        all_f1_scores.append(f1)\n",
    "        all_ndcgs.append(ndcg)\n",
    "\n",
    "        norm_precisions.append(norm_precision)\n",
    "        norm_recalls.append(norm_recall)\n",
    "        norm_f1_scores.append(norm_f1)\n",
    "\n",
    "\n",
    "    # norm_precisions = divide_lists(all_precisions, ideal_precisions)\n",
    "    # norm_recalls = divide_lists(all_recalls, ideal_recalls)\n",
    "    # norm_f1_scores = divide_lists(all_f1_scores, ideal_f1_scores)\n",
    "\n",
    "    # Compute the average evaluation metrics@k across all queries\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_f1_score = np.mean(all_f1_scores)\n",
    "    avg_ndcg = np.mean(all_ndcgs)\n",
    "\n",
    "    norm_avg_precision = np.mean(norm_precisions)\n",
    "    norm_avg_recall = np.mean(norm_recalls)\n",
    "    norm_avg_f1_score = np.mean(norm_f1_scores)\n",
    "\n",
    "    # Print the evaluation metrics@k\n",
    "    print(f\"Metrics@{k}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "    print(f\"Average NDCG: {avg_ndcg:.4f}\\n\")\n",
    "\n",
    "# print(f\"Metrics@{k}\")\n",
    "# print(f\"Average Precision: {norm_avg_precision:.4f}\")\n",
    "# print(f\"Average Recall: {norm_avg_recall:.4f}\")\n",
    "# print(f\"Average F1 Score: {norm_avg_f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e729adf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(judgements.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fb807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
